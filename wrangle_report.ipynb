{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goal\n",
    "This wrangle report describes the process to create trustworthy data for WeRateDogs project analysis. We will go through topics that are necessary for data wrangling:\n",
    "\n",
    " 1. Gathering data\n",
    " 2. Assessing data\n",
    " 3. Cleaning data\n",
    "\n",
    "### Gathering Data\n",
    "There are three parts for gathering data step:\n",
    "\n",
    " 1. The WeRateDogs Twitter archive **twitter-archive-enhanced.csv** <br>\n",
    "    This archive contains basic tweet data with 2,356 records and could be downloaded from the link: [twitter-archive-enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv) \n",
    "    \n",
    "    \n",
    " 2. The Image Predictions File **image_predictions.tsv** <br>\n",
    "    This file is hosted on Udacity's server and contains the dog breed predictions for tweet images using the neural network. The file could be downloaded programmatically using the Requests library in Jupyter Notebook with the URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "    \n",
    "    \n",
    " 3. Additional tweet data through **Twitter API** <br>\n",
    "    Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called **tweet_json.txt file** (Each tweet's JSON data should be written to its own line). Then read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count. \n",
    " \n",
    "### Assessing Data\n",
    "After gathering all the data, we will asses them visually and programmatically for quality and tidiness issues and list them down for cleaning step.\n",
    "\n",
    " 1. Quality: We will idenify content issues (completeness, validity, accuracy, consistency) on each dataset\n",
    "    \n",
    "    **`Twitter archive data` **\n",
    "    \n",
    "    - Retweet data shouldn't be included\n",
    "    - Incorrect `rating_numerator` and `rating_denominator`\n",
    "    - **Doggo, floofer, pupper, puppo** columns should have NaN, not None when it is null\n",
    "    - Inaccurate dog **name**: such, quite, an, a\n",
    "    - Dog **name** should have NaN rather than None when it is null\n",
    "    - Timestamp column should be datetime type\n",
    "    - Missing **expanded_urls** (2,297 instead of 2,356)\n",
    "\n",
    "    **`image_predictions.tsv`**\n",
    "    \n",
    "    - Missing records (2,075 instead of 2,356)\n",
    "    - **p1, p2, p3** should have consistent capitalization\n",
    "\n",
    "    **`tweet_json.txt`**\n",
    "    \n",
    "    - Missing records (2,343 instead of 2,356)\n",
    "\n",
    " 2. Tidiness: We will focus on structure issues for all the dataset\n",
    "    - `image_predictions.tsv` should be part of the `Twitter archive data`\n",
    "    - Favorite count and retweet count in `tweet_json.txt` should be part of the `Twitter archive data`\n",
    "    - **retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp** should be removed since they are for retweet info\n",
    "    - **Doggo, floofer, pupper, puppo** columns should combine into one column **dog_stage**\n",
    "    - **Timestamp** could have extra columns **year, month, weekofDay** to analyze post trends in different time period\n",
    "    \n",
    "### Cleaning Data\n",
    "\n",
    "After identifying the issues for the all datasets, we can start to fix them. During this step, we will create copies for each dataset and manipulate mainly on these new copies. This will help us to keep original datasets and easily compare with the results. Sometimes, we also create temporary columns like `new_rating` and `new_rating1` for manipulation purpose. These temprorary columns along with the duplicate columns like `tweet_id` we got while combining datasets, should be removed in order to present our final result. \n",
    "For each issue, we will use Define, Code, and Test step to document the changes we make. Finally, we will store the clean tidy dataset with 1,991 records as **twitter_archive_master.csv**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
